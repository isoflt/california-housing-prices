{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('anaconda3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the Data\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "fetch_housing_data()\n",
    "\n",
    "# Loading the Data\n",
    "\n",
    "import pandas as pd\n",
    " \n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "\n",
    "housing.info()\n",
    "\n",
    "#housing[\"ocean_proximity\"].value_counts()\n",
    "\n",
    "housing.describe()\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a function to create the test set\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "# Implementing hash computation to split data in train, test set\n",
    "from zlib import crc32\n",
    "\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]\n",
    "\n",
    "housing_with_id = housing.reset_index() # need to add index column since dataset doesn't have one\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\n",
    "\n",
    "# Using latitude and longitude features to build a stable id\n",
    "housing_with_id[\"id\"] = housing[\"longitude\"] *1000 + housing[\"latitude\"]\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")\n",
    "\n",
    "# Using sklearn's train_test_split to create test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure stratified sampling to avoid sampling bias casuing the creation of a non-representative train/test set\n",
    "# Median income is an important attribute to predict the median home prices\n",
    "# Create income categories\n",
    "\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"], \n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "housing[\"income_cat\"].hist()\n",
    "\n",
    "# Create stratified train/test sets\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "\n",
    "# Checking income category proportions to see if split worked\n",
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\n",
    "\n",
    "# Removing the income_cat attribute to return data back to original state\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop (\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "\n",
    "housing = strat_train_set.copy() # Creating a copy of the trianing set to explore without disturbing the actual training set\n",
    "\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1) # High-density areas stand out: Bay Area, Los Angeles, San Diego, Central Valley: Sacramento and Fresno\n",
    "\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, # Regions along the coast with high population density generally have more expensive homes\n",
    "    s=housing[\"population\"]/100, label=\"population\", figsize = (10,7),\n",
    "    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True)\n",
    "plt.legend()\n",
    "\n",
    "from pandas.plotting import scatter_matrix # To plot promising numerical attributes with other numerical attributes to look for correlations\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8)) # The most promising attribute to predict median house value is the median income\n",
    "\n",
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", # Zooming in on the median_income vs median_home_value plot reveals a strong correlation, and a clear upward trend\n",
    "             alpha=0.1)                                                 # Some horizontal lines at different price levels may need to be dealt with to avoid algorithm learning to reproduce them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating some more informative attributes\n",
    "\n",
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"] # Getting the rooms per home as opposed to the whole district\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"] # Getting the ratio of number bedrooms to all rooms\n",
    "housing[\"population_per_household\"] = housing[\"population\"]/housing[\"households\"] # Getting the average number of people living in one home\n",
    "\n",
    "# Checking correlation matrix again to see if the new attributes were informative\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False) # The bedrooms_per_room attribute seems to be much more correlated to the median house value than the total number to bedrooms and                                                                  rooms per district, and appears to be negatively correlated (that is as the number of bedrooms relative to the number of total                                                                    rooms decreases, the higher the median home value). The rooms_per_household attribute also seems to have a reasonable                                                                             correlation to the median home value (that is the larger the house, the more expensive it tends to be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning and general data preparation for applying the algorithms\n",
    "\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "from sklearn.impute import SimpleImputer # Creating an imputer instance to fill in missing values in total_bedrooms attribute\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "imputer.fit(housing_num)\n",
    "imputer.statistics_\n",
    "\n",
    "X = imputer.transform(housing_num) # Fitting imputer to fill in missing values in dataset\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index) # Transforming transformed dataset back into a DataFrame\n",
    "\n",
    "###########\n",
    "\n",
    "housing_cat = housing[[\"ocean_proximity\"]] # Only one categorical attribute\n",
    "housing_cat.head(5)\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder # Converting the tex categories to numerical categories\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10] # One major flaw of this method is when some ML algorithms use distance measures to assign similarity to certain values so categories 0 and 1 will be\n",
    "                         # considered more similar than 0 and 4, when the ordinality of a category does not matter. Another method to use would be one-hot encoding.\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder # Creating instance of one-hot encoder\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot # Sparse matrix due to the large amount of 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer for adding attributes\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAtrributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room=True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, bedrooms_ix] / X[:, population_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "attr_adder = CombinedAtrributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for transformations for all attributes (instead of completing steps one by one)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('attribs_adder', CombinedAtrributesAdder()),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs)\n",
    "    ])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Scores: [69952.92350963 66032.28238426 74735.96803598 74339.50445108\n",
      " 72521.04875707 70577.24128527 72103.27044767 67322.5855089\n",
      " 73882.28500031 70494.66996301]\n",
      "Mean: 71196.17793431657\n",
      "Standard Deviation: 2762.1603746626943\n",
      "Scores: [66271.03705179 66631.22287781 75107.59770393 73108.09877086\n",
      " 67923.30860402 70805.48969234 65075.7320055  67785.94213353\n",
      " 71399.47035119 67097.38137351]\n",
      "Mean: 69120.5280564475\n",
      "Standard Deviation: 3126.700940589302\n",
      "Scores: [50033.90080886 47346.89540929 49756.93776837 53272.76693108\n",
      " 50345.15709479 53073.97900908 48801.08876381 48885.03136826\n",
      " 53624.89660643 50501.2677463 ]\n",
      "Mean: 50564.192150625815\n",
      "Standard Deviation: 2006.669787745942\n"
     ]
    }
   ],
   "source": [
    "# Training the Models\n",
    " \n",
    "from sklearn.linear_model import LinearRegression # Trying vanilla linear regression just to get a basline estimate of a model's performance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse # Model performs quite poorly, typical price of a housing is predicted off by $68,358; model is underfitting\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor # Trying decision trees\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse # Sign of model badly overfitting the data, need to use cross-validation to measure performance better\n",
    "\n",
    "from sklearn.model_selection import cross_val_score # Trying vanilla 10-fold cv\n",
    "\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard Deviation:\", scores.std())\n",
    "display_scores(tree_rmse_scores) # Decision tree not performing that well either, training score is much lower than the validation score, hence still overfitting \n",
    "\n",
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores) # Checking to see Linear Regression performance usign cv, actually performs better than the tree (tree overfitting badly)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor # Trying an ensemble method to see if any improvement in model performance (RF is a relatively good performer)\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
    "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores) # The RF performed much better but the training score is still much lower than the score on the validation sets, model still overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestRegressor(),\n",
       "             param_grid=[{'max_features': [3, 4, 6, 8],\n",
       "                          'n_estimators': [3, 10, 30]},\n",
       "                         {'bootstrap': [False], 'max_features': [2, 3, 4],\n",
       "                          'n_estimators': [3, 10]}],\n",
       "             return_train_score=True, scoring='neg_mean_squared_error')"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# Model Tuning\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [3, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "grid_search.best_params_\n",
    "grid_search.best_estimator_\n",
    "\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)\n",
    "# The best model was obtained by setting the max_features hyperparameter to 8 and the n_estimators parameter to 30, which gave a RMSE score of 50295, slightly better than the default hyperparameters we used for the earlier model. Perhaps try RandomizedSearchCV instead to get an even larger field of hyperparameters, possibly obtaining better hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0.3932638024489401, 'median_income'),\n",
       " (0.14571475857006594, 'INLAND'),\n",
       " (0.10775214882964577, 'pop_per_hhold'),\n",
       " (0.06899322759117771, 'bedrooms_per_room'),\n",
       " (0.0677244900810488, 'longitude'),\n",
       " (0.0640050814711206, 'latitude'),\n",
       " (0.043621086346769804, 'housing_median_age'),\n",
       " (0.035139033305924675, 'rooms_per_hhold'),\n",
       " (0.015876034947622082, 'population'),\n",
       " (0.015525788236839264, 'total_rooms'),\n",
       " (0.01529608745636973, 'households'),\n",
       " (0.014426550659181128, 'total_bedrooms'),\n",
       " (0.007834015539202788, '<1H OCEAN'),\n",
       " (0.002724381711702957, 'NEAR OCEAN'),\n",
       " (0.0020949247586501112, 'NEAR BAY'),\n",
       " (8.588045738466513e-06, 'ISLAND')]"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "# Model Inspection\n",
    "\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)\n",
    "# Interestingly only one ocean-proximity category (INLAND) is useful. Median Income is, predictably, the most useful feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "48043.695875746096"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "# Final Model Evaluation\n",
    "\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test) # Being careful not to fit pipeline on test set\n",
    "\n",
    "final_predicitons = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predicitons)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_rmse # Reasonable result based on validation score"
   ]
  }
 ]
}